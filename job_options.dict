{"-A":"-A, --account=<account>\n   Charge resources used by this job to specified account. The account is an arbitrary string. The account\n   name may be changed after job submission using the scontrol command.",
"--account":"-A, --account=<account>\n   Charge resources used by this job to specified account. The account is an arbitrary string. The account\n   name may be changed after job submission using the scontrol command.",
"-b":"-b, --begin=<time>\n   Submit batch script to the Slurm controller immediately, like normal, but tell the controller to defer the\n   allocation of the job until the specified time.\n   Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). (If that\n   time is already past, the next day is assumed.) You may also specify midnight, noon, elevenses (11 AM),\n   fika (3 PM) or teatime (4 PM) and you can have a time-of-day suffixed with AM or PM for running in the\n   morning or the evening. You can also say what day the job will be run, by specifying a date of the form\n   MMDDYY or MM/DD/YY YYYY-MM-DD. Combine date and time using the following format YYYY-MM-DD[THH:MM[:SS]].\n   You can also give times like now + count time-units, where the time-units can be seconds (default),\n   minutes, hours, days, or weeks and you can tell Slurm to run the job today with the keyword today and to\n   run the job tomorrow with the keyword tomorrow. The value may be changed after\n   job submission using the scontrol command. For example:\n      --begin=16:00\n      --begin=now+1hour\n      --begin=now+60           (seconds by default)\n      --begin=2010-01-20T12:34:00\n   Notes on date/time specifications:\n    - Although the 'seconds' field of the HH:MM:SS time specification is allowed by the code, note that\n      the poll time of the Slurm scheduler is not precise enough to guarantee dispatch of the job on the\n      exact second. The job will be eligible to start on the next poll following the specified time. The exact\n      poll interval depends on the Slurm scheduler (e.g., 60 seconds with the default sched/builtin).\n    - If no time (HH:MM:SS) is specified, the default is (00:00:00)\n    - If a date is specified without a year (e.g., MM/DD) then the current year is assumed, unless the\n      combination of MM/DD and HH:MM:SS has already passed for that year, in which case the next year\n      is used.",
"--begin=":"-b, --begin=<time>\n   Submit batch script to the Slurm controller immediately, like normal, but tell the controller to defer the\n   allocation of the job until the specified time.\n   Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). (If that\n   time is already past, the next day is assumed.) You may also specify midnight, noon, elevenses (11 AM),\n   fika (3 PM) or teatime (4 PM) and you can have a time-of-day suffixed with AM or PM for running in the\n   morning or the evening. You can also say what day the job will be run, by specifying a date of the form\n   MMDDYY or MM/DD/YY YYYY-MM-DD. Combine date and time using the following format YYYY-MM-DD[THH:MM[:SS]].\n   You can also give times like now + count time-units, where the time-units can be seconds (default),\n   minutes, hours, days, or weeks and you can tell Slurm to run the job today with the keyword today and to\n   run the job tomorrow with the keyword tomorrow. The value may be changed after\n   job submission using the scontrol command. For example:\n      --begin=16:00\n      --begin=now+1hour\n      --begin=now+60           (seconds by default)\n      --begin=2010-01-20T12:34:00\n   Notes on date/time specifications:\n    - Although the 'seconds' field of the HH:MM:SS time specification is allowed by the code, note that\n      the poll time of the Slurm scheduler is not precise enough to guarantee dispatch of the job on the\n      exact second. The job will be eligible to start on the next poll following the specified time. The exact\n      poll interval depends on the Slurm scheduler (e.g., 60 seconds with the default sched/builtin).\n    - If no time (HH:MM:SS) is specified, the default is (00:00:00)\n    - If a date is specified without a year (e.g., MM/DD) then the current year is assumed, unless the\n      combination of MM/DD and HH:MM:SS has already passed for that year, in which case the next year\n      is used.",
"-C":"-C, --constraint=<list>\n   Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these\n   features are required by their job using the constraint option. If you are looking for 'soft' constraints\n   please see --prefer for more information. Only nodes having features matching the job constraints will be\n   used to satisfy the request. Multiple constraints may be specified with AND, OR, matching OR, resource\n   counts, etc. (some operators are not supported on all system types).\n   NOTE: Changeable features are features defined by a NodeFeatures plugin.\n   Supported --constraint options include:\n     Single Name\n        Only nodes which have the specified feature will be used. For example, --constraint=\"intel\"\n     Node Count\n       A request can specify the number of nodes needed with some feature by appending an asterisk\n       and count after the feature name. For example, --nodes=16 --constraint=\"graphics*4\"\n       indicates that the job requires 16 nodes and that at least four of those nodes must have the\n       feature \"graphics\" If requesting more than one feature and using node counts, the request must\n       have square brackets surrounding it.\n       NOTE: This option is not supported by the helpers NodeFeatures plugin. Heterogeneous jobs\n       can be used instead.\n     AND\n       Only nodes with all of specified features will be used. The ampersand is used for an AND\n       operator. For example, --constraint=\"intel&gpu\"\n     OR\n       Only nodes with at least one of specified features will be used. The vertical bar is used for an OR\n       operator. If changeable features are not requested, nodes in the allocation can have different\n       features. For example, salloc -N2 --constraint=\"intel|amd\" can result in a job allocation where\n       one node has the intel feature and the other node has the amd feature. However, if the\n       expression contains a changeable feature, then all OR operators are automatically treated as\n       Matching OR so that all nodes in the job allocation have the same set of features. For example,\n       salloc -N2 --constraint=\"foo|bar&baz\" The job is allocated two nodes where both nodes have\n      foo, or bar and baz (one or both nodes could have foo, bar, and baz). The helpers NodeFeatures\n       plugin will find the first set of node features that matches all nodes in the job allocation;\n       these features are set as active features on the node and passed to RebootProgram (see\n       slurm.conf(5)) and the helper script (see helpers.conf(5)). In this case, the helpers plugin uses\n       the first of \"foo\" or \"bar,baz\" that match the two nodes in the job allocation.\n     Matching OR\n       If only one of a set of possible options should be used for all allocated nodes, then use the OR\n       operator and enclose the options within square brackets. For example, --constraint=\"[rack1|\n       rack2|rack3|rack4]\" might be used to specify that all nodes must be allocated on a single\n       rack of the cluster, but any of those four racks can be used.\n     Multiple Counts\n       Specific counts of multiple resources may be specified by using the AND operator and enclosing\n       the options within square brackets. For example, --constraint=\"[rack1*2&rack2*4]\" might be\n       used to specify that two nodes must be allocated from nodes with the feature of \"rack1\" and four\n       nodes must be allocated from nodes with the feature \"rack2\".\n       NOTE: This construct does not support multiple Intel KNL NUMA or MCDRAM modes. For\n       example, while --constraint=\"[(knl&quad)*2&(knl&hemi)*4]\" is not supported, --\n       constraint=\"[haswell*2&(knl&hemi)*4]\" is supported. Specification of multiple KNL modes\n       requires the use of a heterogeneous job.\n       NOTE: This option is not supported by the helpers NodeFeatures plugin.\n       NOTE: Multiple Counts can cause jobs to be allocated with a non-optimal network layout.\n     Brackets\n       Brackets can be used to indicate that you are looking for a set of nodes with the different\n       requirements contained within the brackets. For example, --constraint=\"[(rack1|\n       rack2)*1&(rack3)*2]\" will get you one node with either the \"rack1\" or \"rack2\" features and two\n       nodes with the \"rack3\" feature. If requesting more than one feature and using node counts, the\n       request must have square brackets surrounding it.\n       NOTE: Brackets are only reserved for Multiple Counts and Matching OR syntax. AND\n       operators require a count for each feature inside square brackets (i.e. \"[quad*2&hemi*1]\"). Slurm\n       will only allow a single set of bracketed constraints per job.\n       NOTE: Square brackets are not supported by the helpers NodeFeatures plugin. Matching OR\n       can be requested without square brackets by using the vertical bar character with at least one\n       changeable feature.\n     Parentheses\n       Parentheses can be used to group like node features together. For example, --\n       constraint=\"[(knl&snc4&flat)*4&haswell*1]\" might be used to specify that four nodes with the\n       features \"knl\", \"snc4\" and \"flat\" plus one node with the feature \"haswell\" are required.\n       Parentheses can also be used to group operations. Without parentheses, node features\n       are parsed strictly from left to right. For example, --constraint=\"foo&bar|baz\" requests nodes with\n       foo and bar, or baz. --constraint=\"foo|bar&baz\" requests nodes with foo and baz, or bar and\n       baz (note how baz was AND'd with everything). --constraint=\"foo&(bar|baz)\" requests nodes\n       with foo and at least one of bar or baz.\n       NOTE: OR within parentheses should not be used with a KNL NodeFeatures plugin but is\n       supported by the helpers NodeFeatures plugin.",
"--constraint=":"-C, --constraint=<list>\n   Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these\n   features are required by their job using the constraint option. If you are looking for 'soft' constraints\n   please see --prefer for more information. Only nodes having features matching the job constraints will be\n   used to satisfy the request. Multiple constraints may be specified with AND, OR, matching OR, resource\n   counts, etc. (some operators are not supported on all system types).\n   NOTE: Changeable features are features defined by a NodeFeatures plugin.\n   Supported --constraint options include:\n     Single Name\n        Only nodes which have the specified feature will be used. For example, --constraint=\"intel\"\n     Node Count\n       A request can specify the number of nodes needed with some feature by appending an asterisk\n       and count after the feature name. For example, --nodes=16 --constraint=\"graphics*4\"\n       indicates that the job requires 16 nodes and that at least four of those nodes must have the\n       feature \"graphics\" If requesting more than one feature and using node counts, the request must\n       have square brackets surrounding it.\n       NOTE: This option is not supported by the helpers NodeFeatures plugin. Heterogeneous jobs\n       can be used instead.\n     AND\n       Only nodes with all of specified features will be used. The ampersand is used for an AND\n       operator. For example, --constraint=\"intel&gpu\"\n     OR\n       Only nodes with at least one of specified features will be used. The vertical bar is used for an OR\n       operator. If changeable features are not requested, nodes in the allocation can have different\n       features. For example, salloc -N2 --constraint=\"intel|amd\" can result in a job allocation where\n       one node has the intel feature and the other node has the amd feature. However, if the\n       expression contains a changeable feature, then all OR operators are automatically treated as\n       Matching OR so that all nodes in the job allocation have the same set of features. For example,\n       salloc -N2 --constraint=\"foo|bar&baz\" The job is allocated two nodes where both nodes have\n      foo, or bar and baz (one or both nodes could have foo, bar, and baz). The helpers NodeFeatures\n       plugin will find the first set of node features that matches all nodes in the job allocation;\n       these features are set as active features on the node and passed to RebootProgram (see\n       slurm.conf(5)) and the helper script (see helpers.conf(5)). In this case, the helpers plugin uses\n       the first of \"foo\" or \"bar,baz\" that match the two nodes in the job allocation.\n     Matching OR\n       If only one of a set of possible options should be used for all allocated nodes, then use the OR\n       operator and enclose the options within square brackets. For example, --constraint=\"[rack1|\n       rack2|rack3|rack4]\" might be used to specify that all nodes must be allocated on a single\n       rack of the cluster, but any of those four racks can be used.\n     Multiple Counts\n       Specific counts of multiple resources may be specified by using the AND operator and enclosing\n       the options within square brackets. For example, --constraint=\"[rack1*2&rack2*4]\" might be\n       used to specify that two nodes must be allocated from nodes with the feature of \"rack1\" and four\n       nodes must be allocated from nodes with the feature \"rack2\".\n       NOTE: This construct does not support multiple Intel KNL NUMA or MCDRAM modes. For\n       example, while --constraint=\"[(knl&quad)*2&(knl&hemi)*4]\" is not supported, --\n       constraint=\"[haswell*2&(knl&hemi)*4]\" is supported. Specification of multiple KNL modes\n       requires the use of a heterogeneous job.\n       NOTE: This option is not supported by the helpers NodeFeatures plugin.\n       NOTE: Multiple Counts can cause jobs to be allocated with a non-optimal network layout.\n     Brackets\n       Brackets can be used to indicate that you are looking for a set of nodes with the different\n       requirements contained within the brackets. For example, --constraint=\"[(rack1|\n       rack2)*1&(rack3)*2]\" will get you one node with either the \"rack1\" or \"rack2\" features and two\n       nodes with the \"rack3\" feature. If requesting more than one feature and using node counts, the\n       request must have square brackets surrounding it.\n       NOTE: Brackets are only reserved for Multiple Counts and Matching OR syntax. AND\n       operators require a count for each feature inside square brackets (i.e. \"[quad*2&hemi*1]\"). Slurm\n       will only allow a single set of bracketed constraints per job.\n       NOTE: Square brackets are not supported by the helpers NodeFeatures plugin. Matching OR\n       can be requested without square brackets by using the vertical bar character with at least one\n       changeable feature.\n     Parentheses\n       Parentheses can be used to group like node features together. For example, --\n       constraint=\"[(knl&snc4&flat)*4&haswell*1]\" might be used to specify that four nodes with the\n       features \"knl\", \"snc4\" and \"flat\" plus one node with the feature \"haswell\" are required.\n       Parentheses can also be used to group operations. Without parentheses, node features\n       are parsed strictly from left to right. For example, --constraint=\"foo&bar|baz\" requests nodes with\n       foo and bar, or baz. --constraint=\"foo|bar&baz\" requests nodes with foo and baz, or bar and\n       baz (note how baz was AND'd with everything). --constraint=\"foo&(bar|baz)\" requests nodes\n       with foo and at least one of bar or baz.\n       NOTE: OR within parentheses should not be used with a KNL NodeFeatures plugin but is\n       supported by the helpers NodeFeatures plugin.",
"--cpus-per-gpu":"--cpus-per-gpu=<ncpus>\n   Request that ncpus processors be allocated per allocated GPU. Steps inheriting this value will imply --\n   exact. Not compatible with the --cpus-per-task option.",
"--cpus-per-task":"-c, --cpus-per-task=<ncpus>\n   Advise the Slurm controller that ensuing job steps will require ncpus number of processors per task.\n   Without this option, the controller will just try to allocate one processor per task.\n   For instance, consider an application that has 4 tasks, each requiring 3 processors. If our cluster is\n   comprised of quad-processors nodes and we simply ask for 12 processors, the controller might give us\n   only 3 nodes. However, by using the --cpus-per-task=3 options, the controller knows that each task\n   requires 3 processors on the same node, and the controller will grant an allocation of 4 nodes, one for\n   each of the 4 tasks.",
"-c":"-c, --cpus-per-task=<ncpus>\n   Advise the Slurm controller that ensuing job steps will require ncpus number of processors per task.\n   Without this option, the controller will just try to allocate one processor per task.\n   For instance, consider an application that has 4 tasks, each requiring 3 processors. If our cluster is\n   comprised of quad-processors nodes and we simply ask for 12 processors, the controller might give us\n   only 3 nodes. However, by using the --cpus-per-task=3 options, the controller knows that each task\n   requires 3 processors on the same node, and the controller will grant an allocation of 4 nodes, one for\n   each of the 4 tasks.",
"--error":"-e, --error=<filename_pattern>\n   Instruct Slurm to connect the batch script's standard error directly to the file name specified in the\n   \"filename pattern\". By default both standard output and standard error are directed to the same file. For\n   job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the\n   array index. For other jobs, the default file name is \"slurm-%j.out\", where the \"%j\" is replaced by the job\n   ID. See the filename pattern section below for filename specification options.",
"-e":"-e, --error=<filename_pattern>\n   Instruct Slurm to connect the batch script's standard error directly to the file name specified in the\n   \"filename pattern\". By default both standard output and standard error are directed to the same file. For\n   job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the\n   array index. For other jobs, the default file name is \"slurm-%j.out\", where the \"%j\" is replaced by the job\n   ID. See the filename pattern section below for filename specification options.",
"--exclude":"-x, --exclude=<node_name_list>\n   Explicitly exclude certain nodes from the resources granted to the job.",
"-x":"-x, --exclude=<node_name_list>\n   Explicitly exclude certain nodes from the resources granted to the job.",
"--gpus":"-G, --gpus=[type:]<number>\n   Specify the total number of GPUs required for the job. An optional GPU type specification can be\n   supplied. For example \"--gpus=volta:3\". See also the --gpus-per-node, --gpus-per-socket and --gpus-\n   per-task options.\n   NOTE: The allocation has to contain at least one GPU per node, or one of each GPU type per node if\n   types are used. Use heterogeneous jobs if different nodes need different GPU types.",
"-G":"-G, --gpus=[type:]<number>\n   Specify the total number of GPUs required for the job. An optional GPU type specification can be\n   supplied. For example \"--gpus=volta:3\". See also the --gpus-per-node, --gpus-per-socket and --gpus-\n   per-task options.\n   NOTE: The allocation has to contain at least one GPU per node, or one of each GPU type per node if\n   types are used. Use heterogeneous jobs if different nodes need different GPU types.",
"--gpus-per-node":"--gpus-per-node=[type:]<number>\n   Specify the number of GPUs required for the job on each node included in the job's resource allocation.\n   An optional GPU type specification can be supplied. For example \"--gpus-per-node=volta:3\". Multiple\n   options can be requested in a comma separated list, for example: \"--gpus-per-node=volta:3,kepler:1\".\n   See also the --gpus, --gpus-per-socket and --gpus-per-task options.",
"--gpus-per-socket":"--gpus-per-socket=[type:]<number>\n   Specify the number of GPUs required for the job on each socket included in the job's resource allocation.\n   An optional GPU type specification can be supplied. For example \"--gpus-per-socket=volta:3\". Multiple\n   options can be requested in a comma separated list, for example: \"--gpus-per-socket=volta:3,kepler:1\".\n   Requires job to specify a sockets per node count ( --sockets-per-node). See also the --gpus, --gpus-per-\n   node and --gpus-per-task options.",
"--gpus-per-task":"--gpus-per-task=[type:]<number>\n   Specify the number of GPUs required for the job on each task to be spawned in the job's resource\n   allocation. An optional GPU type specification can be supplied. For example \"--gpus-per-task=volta:1\".\n   Multiple options can be requested in a comma separated list, for example: \"--gpus-per-\n   task=volta:3,kepler:1\". See also the --gpus, --gpus-per-socket and --gpus-per-node options. This\n   option requires an explicit task count, e.g. -n, --ntasks or \"--gpus=X --gpus-per-task=Y\" rather than an\n   ambiguous range of nodes with -N, --nodes. This option will implicitly set --tres-bind=gres/\n   gpu:per_task:<gpus_per_task>, but that can be overridden with an explicit --tres-bind=gres/gpu\n   specification.",
"--gres":"--gres=<list>\n   Specifies a comma-delimited list of generic consumable resources. The format for each entry in the list is\n   \"name[[:type]:count]\". The name is the type of consumable resource (e.g. gpu). The type is an optional\n   classification for the resource (e.g. a100). The count is the number of those resources with a default value\n   of 1. The count can have a suffix of \"k\" or \"K\" (multiple of 1024), \"m\" or \"M\" (multiple of 1024 x 1024), \"g\"\n   or \"G\" (multiple of 1024 x 1024 x 1024), \"t\" or \"T\" (multiple of 1024 x 1024 x 1024 x 1024), \"p\" or \"P\"\n   (multiple of 1024 x 1024 x 1024 x 1024 x 1024). The specified resources will be allocated to the job on\n   each node. The available generic consumable resources is configurable by the system administrator. A\n   list of available generic consumable resources will be printed and the command will exit if the option\n   argument is \"help\". Examples of use include \"--gres=gpu:2\", \"--gres=gpu:kepler:2\", and \"--gres=help\".",
"--mail-type":"--mail-type=<type>\n   Notify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL,\n   REQUEUE, ALL (equivalent to BEGIN, END, FAIL, INVALID_DEPEND, REQUEUE, and STAGE_OUT),\n   INVALID_DEPEND (dependency never satisfied), STAGE_OUT (burst buffer stage out and teardown\n   completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached\n   80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send\n   emails for each array task). Multiple type values may be specified in a comma separated list. NONE will\n   suppress all event notifications, ignoring any other values specified. By default no email notifications are\n   sent. The user to be notified is indicated with --mail-user.\n   Unless the ARRAY_TASKS option is specified, mail notifications on job BEGIN, END, FAIL and\n   REQUEUE apply to a job array as a whole rather than generating individual email messages for each\n   task in the job array.",
"--mail-user":"--mail-user=<user>\n   User to receive email notification of state changes as defined by --mail-type. This may be a full email\n   address or a username. If a username is specified, the value from MailDomain in slurm.conf will be\n   appended to create an email address. The default value is the submitting user.",
"--mem":"--mem=<size>[units]\n   Specify the real memory required per node. Default units are megabytes. Different units can be specified\n   using the suffix [K|M|G|T]. Default value is DefMemPerNode and the maximum value is\n   MaxMemPerNode. If configured, both parameters can be seen using the scontrol show config\n   command. This parameter would generally be used if whole nodes are allocated to jobs\n   (SelectType=select/linear). Also see --mem-per-cpu and --mem-per-gpu. The --mem, --mem-per-cpu\n   and --mem-per-gpu options are mutually exclusive. If --mem, --mem-per-cpu or --mem-per-gpu are\n   specified as command line arguments, then they will take precedence over the environment.\n   NOTE: A memory size specification of zero is treated as a special case and grants the job access to all of\n   the memory on each node.\n   NOTE: Memory requests will not be strictly enforced unless Slurm is configured to use an enforcement\n   mechanism. See ConstrainRAMSpace in the cgroup.conf(5) man page and OverMemoryKill in the\n   slurm.conf(5) man page for more details.",
"--mem-per-cpu":"--mem-per-cpu=<size>[units]\n   Minimum memory required per usable allocated CPU. Default units are megabytes. The default value is\n   DefMemPerCPU and the maximum value is MaxMemPerCPU (see exception below). If configured, both\n   parameters can be seen using the scontrol show config command. Note that if the job's --mem-per-cpu\n   value exceeds the configured MaxMemPerCPU, then the user's limit will be treated as a memory limit per\n   task; --mem-per-cpu will be reduced to a value no larger than MaxMemPerCPU; --cpus-per-task will be\n   set and the value of --cpus-per-task multiplied by the new --mem-per-cpu value will equal the original --\n   mem-per-cpu value specified by the user. If the user already specified a value for --cpus-per-task, it will\n   be respected and only the total amount of allocated CPUs will change. This parameter would generally be\n   used if individual processors are allocated to jobs (SelectType=select/cons_tres). If resources are\n   allocated by core, socket, or whole nodes, then the number of CPUs allocated to a job may be higher\n   than the task count and the value of --mem-per-cpu should be adjusted accordingly. Also see --mem and\n   --mem-per-gpu. The --mem, --mem-per-cpu and --mem-per-gpu options are mutually exclusive.\n   NOTE: If the final amount of memory requested by a job can't be satisfied by any of the nodes configured\n   in the partition, the job will be rejected. This could happen if --mem-per-cpu is used with the --exclusive\n   option for a job allocation and --mem-per-cpu times the number of CPUs on a node is greater than the\n   total memory of that node.\n   NOTE: This applies to usable allocated CPUs in a job allocation. This is important when more than one\n   thread per core is configured. If a job requests --threads-per-core with fewer threads on a core than exist\n   on the core (or --hint=nomultithread which implies --threads-per-core=1), the job will be unable to use\n   those extra threads on the core and those threads will not be included in the memory per CPU\n   calculation. But if the job has access to all threads on the core, those threads will be included in the\n   memory per CPU calculation even if the job did not explicitly request those threads.\n   In the following examples, each core has two threads.\n   In this first example, two tasks can run on separate hyperthreads in the same core because --threads-per-\n   core is not used. The third task uses both threads of the second core. The allocated memory per cpu\n   includes all threads:\n     $ salloc -n3 --mem-per-cpu=100\n     salloc: Granted job allocation 17199\n     $ sacct -j $SLURM_JOB_ID -X -o jobid%7,reqtres%35,alloctres%35\n       JobID                             ReqTRES                           AllocTRES\n     ------- ----------------------------------- -----------------------------------\n       17199     billing=3,cpu=3,mem=300M,node=1     billing=4,cpu=4,mem=400M,node=1\n   In this second example, because of --threads-per-core=1, each task is allocated an entire core but is only\n   able to use one thread per core. Allocated CPUs includes all threads on each core. However, allocated\n   memory per cpu includes only the usable thread in each core.\n     $ salloc -n3 --mem-per-cpu=100 --threads-per-core=1\n     salloc: Granted job allocation 17200\n     $ sacct -j $SLURM_JOB_ID -X -o jobid%7,reqtres%35,alloctres%35\n       JobID                             ReqTRES                           AllocTRES\n     ------- ----------------------------------- -----------------------------------\n       17200     billing=3,cpu=3,mem=300M,node=1     billing=6,cpu=6,mem=300M,node=1",
"--mem-per-gpu":"--mem-per-gpu=<size>[units]\n   Minimum memory required per allocated GPU. Default units are megabytes. Different units can be\n   specified using the suffix [K|M|G|T]. Default value is DefMemPerGPU and is available on both a global\n   and per partition basis. If configured, the parameters can be seen using the scontrol show config and\n   scontrol show partition commands. Also see --mem. The --mem, --mem-per-cpu and --mem-per-gpu\n   options are mutually exclusive.",
"--nodelist":"-w, --nodelist=<node_name_list>\n   Request a specific list of hosts. The job will contain all of these hosts and possibly additional hosts as\n   needed to satisfy resource requirements. The list may be specified as a comma-separated list of hosts, a\n   range of hosts (host[1-5,7,...] for example), or a filename. The host list will be assumed to be a filename if\n   it contains a \"/\" character. If you specify a minimum node or processor count larger than can be satisfied\n   by the supplied host list, additional resources will be allocated on other nodes as needed. Duplicate node\n   names in the list will be ignored. The order of the node names in the list is not important; the node names\n   will be sorted by Slurm.",
"-w":"-w, --nodelist=<node_name_list>\n   Request a specific list of hosts. The job will contain all of these hosts and possibly additional hosts as\n   needed to satisfy resource requirements. The list may be specified as a comma-separated list of hosts, a\n   range of hosts (host[1-5,7,...] for example), or a filename. The host list will be assumed to be a filename if\n   it contains a \"/\" character. If you specify a minimum node or processor count larger than can be satisfied\n   by the supplied host list, additional resources will be allocated on other nodes as needed. Duplicate node\n   names in the list will be ignored. The order of the node names in the list is not important; the node names\n   will be sorted by Slurm.",
"--nodes":"-N, --nodes=<minnodes>[-maxnodes]|<size_string>\n   Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be\n   specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum\n   node count. Node count can be also specified as size_string. The size_string specification identifies what\n   nodes values should be used. Multiple values may be specified using a comma separated list or with a\n   step function by suffix containing a colon and number values with a \"-\" separator. For example, \"--\n   nodes=1-15:4\" is equivalent to \"--nodes=1,5,9,13\". The partition's node limits supersede those of the job.\n   If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a\n   PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a\n   job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that\n   the environment variable SLURM_JOB_NUM_NODES will be set to the count of nodes actually allocated\n   to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the\n   default behavior is to allocate enough nodes to satisfy the requested resources as expressed by per-job\n   specification options, e.g. -n, -c and --gpus. The job will be allocated as many nodes as possible within\n   the range specified and without delaying the initiation of the job. The node count specification may include\n   a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric\n   value by 1,048,576).\n   NOTE: This option cannot be used in with arbitrary distribution.",
"-N":"-N, --nodes=<minnodes>[-maxnodes]|<size_string>\n   Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be\n   specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum\n   node count. Node count can be also specified as size_string. The size_string specification identifies what\n   nodes values should be used. Multiple values may be specified using a comma separated list or with a\n   step function by suffix containing a colon and number values with a \"-\" separator. For example, \"--\n   nodes=1-15:4\" is equivalent to \"--nodes=1,5,9,13\". The partition's node limits supersede those of the job.\n   If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a\n   PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a\n   job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that\n   the environment variable SLURM_JOB_NUM_NODES will be set to the count of nodes actually allocated\n   to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the\n   default behavior is to allocate enough nodes to satisfy the requested resources as expressed by per-job\n   specification options, e.g. -n, -c and --gpus. The job will be allocated as many nodes as possible within\n   the range specified and without delaying the initiation of the job. The node count specification may include\n   a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric\n   value by 1,048,576).\n   NOTE: This option cannot be used in with arbitrary distribution.",
"--ntasks":"-n, --ntasks=<number>\n   sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This\n   option advises the Slurm controller that job steps run within the allocation will launch a maximum of\n   number tasks and to provide for sufficient resources. The default is one task per node, but note that the --\n   cpus-per-task option will change this default.",
"-n":"-n, --ntasks=<number>\n   sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This\n   option advises the Slurm controller that job steps run within the allocation will launch a maximum of\n   number tasks and to provide for sufficient resources. The default is one task per node, but note that the --\n   cpus-per-task option will change this default.",
"--ntasks-per-core":"--ntasks-per-core=<ntasks>\n   Request the maximum ntasks be invoked on each core. Meant to be used with the --ntasks option.\n   Related to --ntasks-per-node except at the core level instead of the node level. This option will be\n   inherited by srun. Slurm may allocate more cpus than what was requested in order to respect this option.\n   NOTE: This option is not supported when using SelectType=select/linear. This value can not be greater\n   than --threads-per-core.",
"--ntasks-per-gpu":"--ntasks-per-gpu=<ntasks>\n   Request that there are ntasks tasks invoked for every GPU. This option can work in two ways: 1) either\n   specify --ntasks in addition, in which case a type-less GPU specification will be automatically determined\n   to satisfy --ntasks-per-gpu, or 2) specify the GPUs wanted (e.g. via --gpus or --gres) without specifying\n   --ntasks, and the total task count will be automatically determined. The number of CPUs needed will be\n   automatically increased if necessary to allow for any calculated task count. This option will implicitly set --\n   tres-bind=gres/gpu:single:<ntasks>, but that can be overridden with an explicit --tres-bind=gres/gpu\n   specification. This option is not compatible with a node range (i.e. -N<minnodes-maxnodes>). This option\n   is not compatible with --gpus-per-task, --gpus-per-socket, or --ntasks-per-node. This option is not\n   supported unless SelectType=cons_tres is configured (either directly or indirectly on Cray systems).",
"--ntasks-per-node":"--ntasks-per-node=<ntasks>\n   Request that ntasks be invoked on each node. If used with the --ntasks option, the --ntasks option will\n   take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node.\n   Meant to be used with the --nodes option. This is related to --cpus-per-task=ncpus, but does not require\n   knowledge of the actual number of cpus on each node. In some cases, it is more convenient to be able to\n   request that no more than a specific number of tasks be invoked on each node. Examples of this include\n   submitting a hybrid MPI/OpenMP app where only one MPI \"task/rank\" should be assigned to each node\n   while allowing the OpenMP portion to utilize all of the parallelism present in the node, or submitting a\n   single setup/cleanup/monitoring job to each node of a pre-existing allocation as one step in a larger job\n   script.",
"--ntasks-per-socket":"--ntasks-per-socket=<ntasks>\n   Request the maximum ntasks be invoked on each socket. Meant to be used with the --ntasks option.\n   Related to --ntasks-per-node except at the socket level instead of the node level. NOTE: This option is\n   not supported when using SelectType=select/linear.",
"--output":"-o, --output=<filename_pattern>\n   Instruct Slurm to connect the batch script's standard output directly to the file name specified in the\n   \"filename pattern\". By default both standard output and standard error are directed to the same file. For\n   job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the\n   array index. For other jobs, the default file name is \"slurm-%j.out\", where the \"%j\" is replaced by the job\n   ID. See the filename pattern section below for filename specification options.",
"-o":"-o, --output=<filename_pattern>\n   Instruct Slurm to connect the batch script's standard output directly to the file name specified in the\n   \"filename pattern\". By default both standard output and standard error are directed to the same file. For\n   job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the\n   array index. For other jobs, the default file name is \"slurm-%j.out\", where the \"%j\" is replaced by the job\n   ID. See the filename pattern section below for filename specification options.",
"--partition":"-p, --partition=<partition_names>\n   Request a specific partition for the resource allocation. If not specified, the default behavior is to allow the\n   slurm controller to select the default partition as designated by the system administrator. If the job can use\n   more than one partition, specify their names in a comma separate list and the one offering earliest\n   initiation will be used with no regard given to the partition name ordering (although higher priority\n   partitions will be considered first). When the job is initiated, the name of the partition used will be placed\n   first in the job record partition string.",
"-p":"-p, --partition=<partition_names>\n   Request a specific partition for the resource allocation. If not specified, the default behavior is to allow the\n   slurm controller to select the default partition as designated by the system administrator. If the job can use\n   more than one partition, specify their names in a comma separate list and the one offering earliest\n   initiation will be used with no regard given to the partition name ordering (although higher priority\n   partitions will be considered first). When the job is initiated, the name of the partition used will be placed\n   first in the job record partition string.",
"--prefer":"--prefer=<list>\n   Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these\n   features are desired but not required by their job using the prefer option. This option operates\n   independently from --constraint and will override whatever is set there if possible. When scheduling, the\n   features in --prefer are tried first. If a node set isn't available with those features then --constraint is\n   attempted. See --constraint for more information, this option behaves the same way.",
"--qos":"-q, --qos=<qos>\n   Request a quality of service for the job. QOS values can be defined for each user/cluster/account\n   association in the Slurm database. Users will be limited to their association's defined set of qos's when\n   the Slurm configuration parameter, AccountingStorageEnforce, includes \"qos\" in its definition.",
"-q":"-q, --qos=<qos>\n   Request a quality of service for the job. QOS values can be defined for each user/cluster/account\n   association in the Slurm database. Users will be limited to their association's defined set of qos's when\n   the Slurm configuration parameter, AccountingStorageEnforce, includes \"qos\" in its definition.",
"--reservation":"--reservation=<reservation_names>\n   Allocate resources for the job from the named reservation. If the job can use more than one reservation,\n   specify their names in a comma separate list and the one offering earliest initiation. Each reservation will\n   be considered in the order it was requested. All reservations will be listed in scontrol/squeue through the\n   life of the job. In accounting the first reservation will be seen and after the job starts the reservation used\n   will replace it.",
"--threads-per-core":"--threads-per-core=<threads>\n   Restrict node selection to nodes with at least the specified number of threads per core. In task layout, use\n   the specified maximum number of threads per core. NOTE: \"Threads\" refers to the number of processing\n   units on each core rather than the number of application tasks to be launched per core. See additiona\n   information under -B option above when task/affinity plugin is enabled.\n   NOTE: This option may implicitly set the number of tasks (if -n was not specified) as one task per\n   requested thread.",
"--time":"-t, --time=<time>\n   Set a limit on the total run time of the job allocation. If the requested time limit exceeds the partition's time\n   limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's\n   default time limit. When the time limit is reached, each task in each job step is sent SIGTERM followed by\n   SIGKILL. The interval between signals is specified by the Slurm configuration parameter KillWait. The\n   OverTimeLimit configuration parameter may permit the job to run longer than scheduled. Time resolution\n   is one minute and second values are rounded up to the next minute.\n   A time limit of zero requests that no time limit be imposed. Acceptable time formats include \"minutes\",\n   \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-\n   hours:minutes:seconds\".",
"-t":"-t, --time=<time>\n   Set a limit on the total run time of the job allocation. If the requested time limit exceeds the partition's time\n   limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's\n   default time limit. When the time limit is reached, each task in each job step is sent SIGTERM followed by\n   SIGKILL. The interval between signals is specified by the Slurm configuration parameter KillWait. The\n   OverTimeLimit configuration parameter may permit the job to run longer than scheduled. Time resolution\n   is one minute and second values are rounded up to the next minute.\n   A time limit of zero requests that no time limit be imposed. Acceptable time formats include \"minutes\",\n   \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-\n   hours:minutes:seconds\".",
"--tmp":"--tmp=<size>[units]\n   Specify a minimum amount of temporary disk space per node. Default units are megabytes. Different\n   units can be specified using the suffix [K|M|G|T].",
"--tres-per-task":"--tres-per-task=<list>\n   Specifies a comma-delimited list of trackable resources required for the job on each task to be spawned\n   in the job's resource allocation. The format for each entry in the list is \"trestype[/tresname]:count\". The\n   trestype is the type of trackable resource requested (e.g. cpu, gres, license, etc). The tresname is the\n   for tres types such as gres, license, etc. (e.g. gpu, gpu:a100). In order to request a license with this\n   option, the license(s) must be defined in the AccountingStorageTRES parameter of slurm.conf. The\n   count is the number of those resources.\n   The count can have a suffix of\n   \"k\" or \"K\" (multiple of 1024),\n   \"m\" or \"M\" (multiple of 1024 x 1024),\n   \"g\" or \"G\" (multiple of 1024 x 1024 x 1024),\n   \"t\" or \"T\" (multiple of 1024 x 1024 x 1024 x 1024),\n   \"p\" or \"P\" (multiple of 1024 x 1024 x 1024 x 1024 x 1024).\n   Examples:\n     --tres-per-task=cpu:4\n     --tres-per-task=cpu:8,license/ansys:1\n     --tres-per-task=gres/gpu:1\n     --tres-per-task=gres/gpu:a100:2\n   The specified resources will be allocated to the job on each node. The available trackable resources are\n   configurable by the system administrator.\n   NOTE: This option with gres/gpu or gres/shard will implicitly set --tres-bind=per_task:(gpu or\n   shard)<tres_per_task>; this can be overridden with an explicit --tres-bind specification.\n   NOTE: Invalid TRES for --tres-per-task include bb,billing,energy,fs,mem,node,pages,vmem."}
